{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "ce2e03ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\thelu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "2e50afd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    " \n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# filter out stop words\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\ttokens = [w for w in tokens if not w in stop_words]\n",
    "\t# filter out short tokens\n",
    "\ttokens = [word for word in tokens if len(word) > 1]\n",
    "\treturn tokens\n",
    " \n",
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "\t# load doc\n",
    "\tdoc = load_doc(filename)\n",
    "\t# clean doc\n",
    "\ttokens = clean_doc(doc)\n",
    "\t# update counts\n",
    "\tvocab.update(tokens)\n",
    "\n",
    "# load num amount of docs in a directory\n",
    "def process_docs(directory, vocab, start, end):\n",
    "\t# walk through all files in the folder\n",
    "\tfor filename in listdir(directory)[start:end]:\n",
    "\t\t# skip files that do not have the right extension\n",
    "\t\tif not filename.endswith(\".txt\"):\n",
    "\t\t\tcontinue\n",
    "\t\t# create the full path of the file to open\n",
    "\t\tpath = directory + '/' + filename\n",
    "\t\t# add doc to vocab\n",
    "\t\tadd_doc_to_vocab(path, vocab)\n",
    "            \n",
    "\n",
    "# save list to file\n",
    "def save_list(lines, filename):\n",
    "\tdata = '\\n'.join(lines)\n",
    "\tfile = open(filename, 'w')\n",
    "\tfile.write(data)\n",
    "\tfile.close()\n",
    " \n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "# training set size: 70/30 split\n",
    "neg_size = len(listdir('neg'))\n",
    "neg_train_size = int(neg_size * 0.7)\n",
    "pos_size = len(listdir('pos'))\n",
    "pos_train_size = int(pos_size * 0.7)\n",
    "# add all docs to vocab\n",
    "process_docs('neg', vocab, 0, neg_train_size)\n",
    "process_docs('pos', vocab, 0, pos_train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "a9a73f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename, vocab):\n",
    "\t# load the doc\n",
    "\tdoc = load_doc(filename)\n",
    "\t# clean doc\n",
    "\ttokens = clean_doc(doc)\n",
    "\t# filter by vocab\n",
    "\ttokens = [w for w in tokens if w in vocab]\n",
    "\treturn ' '.join(tokens)\n",
    "\n",
    "# Updated process_docs for pos and neg lines of words\n",
    "def process_docs(directory, vocab, start, end):\n",
    "\tlines = list()\n",
    "\t# walk through all files in the folder\n",
    "\tfor filename in listdir(directory)[start:end]:\n",
    "\t\t# skip files that do not have the right extension\n",
    "\t\tif not filename.endswith(\".txt\"):\n",
    "\t\t\tcontinue\n",
    "\t\t# create the full path of the file to open\n",
    "\t\tpath = directory + '/' + filename\n",
    "\t\t# load and clean the doc\n",
    "\t\tline = doc_to_line(path, vocab)\n",
    "\t\t# add to list\n",
    "\t\tlines.append(line)\n",
    "\treturn lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "61f3c03b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1260\n",
      "540\n",
      "1260\n",
      "540\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#Ref: https://github.com/Vakhshoori101/TwitterSentimentAnalysis/blob/main/Twitter%20Classification%20Template.ipynb\n",
    "# 70% Training: 630 out of 900, 30% Testing: 270 out of 900\n",
    "neg_train_lines = process_docs('neg', vocab, 0, neg_train_size)\n",
    "neg_test_lines = process_docs('neg', vocab, neg_train_size, neg_size)\n",
    "pos_train_lines = process_docs('pos', vocab, 0, pos_train_size)\n",
    "pos_test_lines = process_docs('pos', vocab, pos_train_size, pos_size)\n",
    "\n",
    "train_x = neg_train_lines + pos_train_lines\n",
    "test_x = neg_test_lines + pos_test_lines\n",
    "\n",
    "train_y = np.append(np.ones(len(pos_train_lines)), np.zeros(len(neg_train_lines)))\n",
    "test_y = np.append(np.ones(len(pos_test_lines)), np.zeros(len(neg_test_lines)))\n",
    "\n",
    "print(len(train_x))\n",
    "print(len(test_x))\n",
    "print(len(train_y))\n",
    "print(len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "a2a3b026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary for the frequency of words appears in number of positive reviews\n",
    "pos_freq_list = {}\n",
    "# create a dictionary for the frequency of words appears in number of negative reviews\n",
    "neg_freq_list = {}\n",
    "\n",
    "#set the default appreance for each word to be 15 to prevent special case 0\n",
    "for token in vocab:\n",
    "    pos_freq_list[token] = 4\n",
    "    neg_freq_list[token] = 4\n",
    "\n",
    "#This function will tokenize each review in the directory and add one to the counter if it appears in the review\n",
    "def count_freq(directory, freq_list, num):\n",
    "\tfor i, filename in enumerate(listdir(directory)):\n",
    "\t\tif i < num:\n",
    "\t\t\tif not filename.endswith(\".txt\"):\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tpath = directory + '/' + filename\n",
    "\t\t\tdoc = load_doc(path)\n",
    "\t\t\ttemp_token = Counter()\n",
    "\t\t\ttok = clean_doc(doc)\n",
    "\t\t\ttemp_token.update(tok)\n",
    "\t\t\tfor w in temp_token:\n",
    "\t\t\t\tfreq_list[w] += 1\n",
    "\treturn freq_list\n",
    "\n",
    "#update the list for both pos and neg frequency\n",
    "neg_freq_list = count_freq('neg', neg_freq_list, neg_train_size)\n",
    "pos_freq_list = count_freq('pos', pos_freq_list, pos_train_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "outputs": [],
   "source": [
    "from math import log2\n",
    "\n",
    "# Default number for total training review is 1260\n",
    "# positive and negative training size are both 630\n",
    "pos = pos_train_size\n",
    "neg = neg_train_size\n",
    "N = pos + neg\n",
    "\n",
    "neg_I = {}\n",
    "pos_I = {}\n",
    "\n",
    "min_occur = 60\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occur]\n",
    "\n",
    "# Mutual information for all possible tokens in negative reviews\n",
    "for token in tokens:\n",
    "    neg_I[token] = log2((neg_freq_list[token]*N)/((neg_freq_list[token]+pos_freq_list[token])*neg))\n",
    "\n",
    "\n",
    "# Mutual information for all possible tokens in positive reviews\n",
    "for token in tokens:\n",
    "    pos_I[token] = log2((pos_freq_list[token]*N)/((neg_freq_list[token]+pos_freq_list[token])*pos))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1: Word: terrific\n",
      "freq: 0.6256044852185021\n",
      "P2: Word: subtle\n",
      "freq: 0.6175303631775868\n",
      "P3: Word: excellent\n",
      "freq: 0.6141088463806726\n",
      "P4: Word: memorable\n",
      "freq: 0.5908873346782616\n",
      "P5: Word: period\n",
      "freq: 0.5767885692754561\n",
      "N1: Word: wasted\n",
      "freq: 0.7476128383657147\n",
      "N2: Word: waste\n",
      "freq: 0.7104933828050153\n",
      "N3: Word: worst\n",
      "freq: 0.7094848577440992\n",
      "N4: Word: awful\n",
      "freq: 0.7030182622428686\n",
      "N5: Word: lame\n",
      "freq: 0.7004397181410922\n"
     ]
    }
   ],
   "source": [
    "# sorted the positive mutal information and get the top 5\n",
    "top_pos = sorted(pos_I.items(), key=lambda x:-x[1])[:5]\n",
    "# sorted the negative mutal information and get the top 5\n",
    "top_neg = sorted(neg_I.items(), key=lambda x:-x[1])[:5]\n",
    "\n",
    "for x in range(5):\n",
    "    print(f\"P{str(x+1)}: Word: {top_pos[x][0]}\\nfreq: {top_pos[x][1]}\")\n",
    "\n",
    "\n",
    "for x in range(5):\n",
    "    print(f\"N{str(x+1)}: Word: {top_neg[x][0]}\\nfreq: {top_neg[x][1]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}