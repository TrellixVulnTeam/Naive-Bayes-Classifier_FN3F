{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce2e03ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\thelu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2e50afd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    " \n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# filter out stop words\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\ttokens = [w for w in tokens if not w in stop_words]\n",
    "\t# filter out short tokens\n",
    "\ttokens = [word for word in tokens if len(word) > 1]\n",
    "\treturn tokens\n",
    " \n",
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename):\n",
    "\t# load doc\n",
    "\tdoc = load_doc(filename)\n",
    "\t# clean doc\n",
    "\ttokens = clean_doc(doc)\n",
    "\t# update counts\n",
    "\tvocab.update(tokens)\n",
    "\treturn tokens\n",
    "\n",
    "# loads all docs in the directory\n",
    "def process_docs1(directory):\n",
    "\tall_reviews = {}\n",
    "\t# walk through all files in the folder\n",
    "\tfor filename in listdir(directory):\n",
    "\t\t# skip files that do not have the right extension\n",
    "\t\tif not filename.endswith(\".txt\"):\n",
    "\t\t\tcontinue\n",
    "\t\t# create the full path of the file to open\n",
    "\t\tpath = directory + '/' + filename\n",
    "\t\t# add doc to list\n",
    "\t\tall_reviews[filename] = add_doc_to_vocab(path)\n",
    "\n",
    "\treturn all_reviews\n",
    "\n",
    "def subset_docs(directory, start, end):\n",
    "\tsubset = []\n",
    "\tfor filename in listdir(directory)[start:end]:\n",
    "\t\t# skip files that do not have the right extension\n",
    "\t\tif not filename.endswith(\".txt\"):\n",
    "\t\t\tcontinue\n",
    "\t\t# create the full path of the file to open\n",
    "\t\tpath = directory + '/' + filename\n",
    "\t\t# add doc to list\n",
    "\t\tsubset.append(add_doc_to_vocab(path))\n",
    "\treturn subset\n",
    "\n",
    "# # save list to file\n",
    "# def save_list(lines, filename):\n",
    "# \tdata = '\\n'.join(lines)\n",
    "# \tfile = open(filename, 'w')\n",
    "# \tfile.write(data)\n",
    "# \tfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a9a73f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename, vocab):\n",
    "\t# load the doc\n",
    "\tdoc = load_doc(filename)\n",
    "\t# clean doc\n",
    "\ttokens = clean_doc(doc)\n",
    "\t# filter by vocab\n",
    "\ttokens = [w for w in tokens if w in vocab]\n",
    "\treturn ' '.join(tokens)\n",
    "\n",
    "# load num amount of docs in a directory\n",
    "def process_docs(directory, vocab, start, end):\n",
    "\t# walk through all files in the folder\n",
    "\tfor filename in listdir(directory)[start:end]:\n",
    "\t\t# skip files that do not have the right extension\n",
    "\t\tif not filename.endswith(\".txt\"):\n",
    "\t\t\tcontinue\n",
    "\t\t# create the full path of the file to open\n",
    "\t\tpath = directory + '/' + filename\n",
    "\t\t# add doc to vocab\n",
    "\t\tadd_doc_to_vocab(path, vocab)\n",
    "\n",
    "vocab = Counter()\n",
    "\n",
    "# Updated process_docs for pos and neg lines of words\n",
    "def process_docs2(directory, vocab, start, end):\n",
    "\tlines = list()\n",
    "\t# walk through all files in the folder\n",
    "\tfor filename in listdir(directory)[start:end]:\n",
    "\t\t# skip files that do not have the right extension\n",
    "\t\tif not filename.endswith(\".txt\"):\n",
    "\t\t\tcontinue\n",
    "\t\t# create the full path of the file to open\n",
    "\t\tpath = directory + '/' + filename\n",
    "\t\t# load and clean the doc\n",
    "\t\tline = doc_to_line(path, vocab)\n",
    "\t\t# add to list\n",
    "\t\tlines.append(line)\n",
    "\treturn lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "61f3c03b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1260\n",
      "540\n",
      "1260\n",
      "540\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "#Ref: https://github.com/Vakhshoori101/TwitterSentimentAnalysis/blob/main/Twitter%20Classification%20Template.ipynb\n",
    "# 70% Training: 630 out of 900, 30% Testing: 270 out of 900\n",
    "neg_size = len(listdir('neg'))\n",
    "neg_train_size = int(neg_size * 0.7)\n",
    "pos_size = len(listdir('pos'))\n",
    "pos_train_size = int(pos_size * 0.7)\n",
    "\n",
    "neg_train_lines = subset_docs('neg', 0, neg_train_size)\n",
    "neg_test_lines = subset_docs('neg', neg_train_size, neg_size)\n",
    "pos_train_lines = subset_docs('pos', 0, pos_train_size)\n",
    "pos_test_lines = subset_docs('pos', pos_train_size, pos_size)\n",
    "\n",
    "train_x = neg_train_lines + pos_train_lines\n",
    "test_x = neg_test_lines + pos_test_lines\n",
    "\n",
    "train_y = np.append(np.ones(len(pos_train_lines)), np.zeros(len(neg_train_lines)))\n",
    "test_y = np.append(np.ones(len(pos_test_lines)), np.zeros(len(neg_test_lines)))\n",
    "\n",
    "mapIndexPos = list(zip(train_x, train_y))\n",
    "random.shuffle(mapIndexPos)\n",
    "train_x_shuffled, train_y_shuffled = list(zip(*mapIndexPos))\n",
    "\n",
    "print(len(train_x_shuffled))\n",
    "print(len(test_x))\n",
    "print(len(train_y_shuffled))\n",
    "print(len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a2a3b026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary for the frequency of words appears in number of positive reviews\n",
    "pos_freq_list = {}\n",
    "# create a dictionary for the frequency of words appears in number of negative reviews\n",
    "neg_freq_list = {}\n",
    "\n",
    "#set the default appreance for each word to be 15 to prevent special case 0\n",
    "for token in vocab:\n",
    "    pos_freq_list[token] = 4\n",
    "    neg_freq_list[token] = 4\n",
    "\n",
    "#This function will tokenize each review in the directory up to num and add one to the counter if it appears in the review\n",
    "def count_freq(directory, freq_list, num):\n",
    "\tfor i, filename in enumerate(listdir(directory)):\n",
    "\t\tif i < num:\n",
    "\t\t\tif not filename.endswith(\".txt\"):\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tpath = directory + '/' + filename\n",
    "\t\t\tdoc = load_doc(path)\n",
    "\t\t\ttemp_token = Counter()\n",
    "\t\t\ttok = clean_doc(doc)\n",
    "\t\t\ttemp_token.update(tok)\n",
    "\t\t\tfor w in temp_token:\n",
    "\t\t\t\tfreq_list[w] += 1\n",
    "\treturn freq_list\n",
    "\n",
    "#update the list for both pos and neg frequency\n",
    "neg_freq_list = count_freq('neg', neg_freq_list, neg_train_size)\n",
    "pos_freq_list = count_freq('pos', pos_freq_list, pos_train_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f98af8ac",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from math import log2\n",
    "\n",
    "# Default number for total training review is 1260\n",
    "# positive and negative training size are both 630\n",
    "pos = pos_train_size\n",
    "neg = neg_train_size\n",
    "N = pos + neg\n",
    "\n",
    "neg_I = {}\n",
    "pos_I = {}\n",
    "\n",
    "min_occur = 200\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occur]\n",
    "\n",
    "# Mutual information for all possible tokens in negative reviews\n",
    "for token in tokens:\n",
    "    neg_I[token] = log2((neg_freq_list[token]*N)/((neg_freq_list[token]+pos_freq_list[token])*neg))\n",
    "\n",
    "\n",
    "# Mutual information for all possible tokens in positive reviews\n",
    "for token in tokens:\n",
    "    pos_I[token] = log2((pos_freq_list[token]*N)/((neg_freq_list[token]+pos_freq_list[token])*pos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d962b4e1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "top_num = 25\n",
    "\n",
    "# sorted the positive mutal information and get the top 5\n",
    "top_pos = sorted(pos_I.items(), key=lambda x:-x[1])[:top_num]\n",
    "# sorted the negative mutal information and get the top 5\n",
    "top_neg = sorted(neg_I.items(), key=lambda x:-x[1])[:top_num]\n",
    "\n",
    "features = top_pos + top_neg\n",
    "feature_words = []\n",
    "for word, p in features:\n",
    "\tfeature_words.append(word)\n",
    "\n",
    "# for x in range(top_num):\n",
    "#     print(f\"P{str(x+1)}: Word: {top_pos[x][0]}\\nfreq: {top_pos[x][1]}\")\n",
    "#\n",
    "#\n",
    "# for x in range(top_num):\n",
    "#         print(f\"N{str(x+1)}: Word: {top_neg[x][0]}\\nfreq: {top_neg[x][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b7ecdad6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# takes in a single review and returns frequency dictionary\n",
    "def create_frequency(reviews, ys):\n",
    "\tfreq_d = {}\n",
    "\n",
    "\t# create freq dictionary\n",
    "\tfor review, y in zip(reviews, ys):\n",
    "\t\tfor word in review:\n",
    "\t\t\tpair = (word, y)\n",
    "\t\t\t# if already in dictionary, add 1\n",
    "\t\t\tif pair in freq_d:\n",
    "\t\t\t\tfreq_d[pair] += 1\n",
    "\t\t\t# add entry to dictionary if not present\n",
    "\t\t\telse:\n",
    "\t\t\t\tfreq_d[pair] = freq_d.get(pair, 1)\n",
    "\n",
    "\treturn freq_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2a2f3490",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# has been saved into pickle file\n",
    "# freqs = create_frequency(train_x_shuffled, train_y_shuffled)\n",
    "\n",
    "def train_naive_bayes(freq, train_x, train_y):\n",
    "\n",
    "\tloglikelihood = {}\n",
    "\tlogprior = 0\n",
    "\t# calculate number of unique words in vocab\n",
    "\tunique_words = set([pair[0] for pair in freq.keys()])\n",
    "\tV = len(unique_words)\n",
    "\t# calculate N_pos and N_neg\n",
    "\tN_pos = N_neg = 0\n",
    "\tfor pair in freq.keys():\n",
    "\t\tif pair[1] > 0:\n",
    "\t\t\tN_pos += freq[(pair)]\n",
    "\t\telse:\n",
    "\t\t\tN_neg += freq[(pair)]\n",
    "\n",
    "\t# number of reviews\n",
    "\tD = len(train_y_shuffled)\n",
    "\t# number of pos reviews\n",
    "\tD_pos = sum(train_y_shuffled)\n",
    "\t# number of neg reviews\n",
    "\tD_neg = D - D_pos\n",
    "\n",
    "\tlogprior = np.log(D_pos) - np.log(D_neg)\n",
    "\n",
    "\t# for each selected feature\n",
    "\tfor word in feature_words:\n",
    "\t\t# get the pos and neg freq of the word\n",
    "\t\tpos_freq = freq.get((word, 1), 0)\n",
    "\t\tneg_freq = freq.get((word, 0), 0)\n",
    "\t\t# calculate probability that word is pos and neg\n",
    "\t\tp_word_pos = (pos_freq + 1) / (N_pos + V)\n",
    "\t\tp_word_neg = (neg_freq + 1) / (N_neg + V)\n",
    "\t\t# calculate the log likelihood of the word\n",
    "\t\tloglikelihood[word] = np.log(p_word_pos / p_word_neg)\n",
    "\n",
    "\treturn logprior, loglikelihood\n",
    "# has been saved into pickle file\n",
    "# logp, logl = train_naive_bayes(freqs, train_x_shuffled, train_y_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "67323fe3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def naive_bayes_predict(review, logprior, loglikelihood):\n",
    "\t# process review to get list of words\n",
    "\n",
    "\t# initialize probability to zero\n",
    "\tp = 0\n",
    "\t# add logprior\n",
    "\tp += logprior\n",
    "\n",
    "\tfor word in review:\n",
    "\t\tif word in loglikelihood:\n",
    "\t\t\tp += loglikelihood[word]\n",
    "\n",
    "\treturn p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e3a4305f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7738095238095238\n",
      "0.7648148148148148\n"
     ]
    }
   ],
   "source": [
    "# save logprior and likelihood into pickle file\n",
    "# save_logprior = open(\"logprior.pickle\", \"wb\")\n",
    "# pickle.dump(logp, save_logprior)\n",
    "# save_logprior.close()\n",
    "#\n",
    "# save_loglikelihood = open(\"loglikelihood.pickle\", \"wb\")\n",
    "# pickle.dump(logl, save_loglikelihood)\n",
    "# save_loglikelihood.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e6795f15",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# returns filename and whether or not review is classified as pos (1)\n",
    "def run_bayes(x_set, y_set, logp, logl):\n",
    "\tcorrect = 0\n",
    "\t# review is path\n",
    "\tfor review, y in zip(x_set, y_set):\n",
    "\t\tpredicted = 1\n",
    "\t\tp = naive_bayes_predict(review, logp, logl)\n",
    "\t\t# print(f'p:{p}')\n",
    "\t\tif p < 0:\n",
    "\t\t\tpredicted = 0\n",
    "\t\tif predicted == y:\n",
    "\t\t\tcorrect += 1\n",
    "\t\t# print(f'predicted: {predicted} actual:{y}')\n",
    "\tprint(correct/len(y_set))\n",
    "\n",
    "with open(\"logprior.pickle\", \"rb\") as logp_f:\n",
    "\tlogp = pickle.load(logp_f)\n",
    "\n",
    "with open(\"loglikelihood.pickle\", \"rb\") as logl_f:\n",
    "\tlogl = pickle.load(logl_f)\n",
    "\n",
    "run_bayes(train_x, train_y, logp, logl)\n",
    "run_bayes(test_x, test_y, logp, logl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "62b22313",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def new_bayes(directory, logp, logl):\n",
    "\tcorrect = 0\n",
    "\tpredict_list = []\n",
    "\t# review is path\n",
    "\treviews = process_docs1(directory)\n",
    "\tfor key in reviews.keys():\n",
    "\t\tpos = 1\n",
    "\t\tp = naive_bayes_predict(reviews[key], logp, logl)\n",
    "\t\tif p < 0:\n",
    "\t\t\tpos = 0\n",
    "\t\tpredict_list.append(f'{key},{pos}')\n",
    "\n",
    "\treturn predict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e4a0abf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "dir = input(\"Enter directory name: \")\n",
    "write_me = new_bayes(dir, logp, logl)\n",
    "# open file to write\n",
    "def write_to_csv(prediction_list):\n",
    "\twith open('results.csv', 'w', newline='') as f:\n",
    "\t\tfor prediction in prediction_list:\n",
    "\t\t\tf.write(prediction + '\\n')\n",
    "\n",
    "write_to_csv(write_me)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}